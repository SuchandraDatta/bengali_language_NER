{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\n!pip install tokenizers\n!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><b>NER on Bengali Language</b></h3>\n<h3>Things to notice</h3>\n<b>\n1. The model checkpoint should be mbert</br>\n2. We need to use mbert cased version (new, recommended) not the uncased version ( refer to https://github.com/google-research/bert/blob/master/multilingual.md )</br>\n3. In the uncased version, there are normalization issues for bengali, which I saw as the spelling of input word and decoded word is different</br>\n4. The train loss is greater than eval loss in very initial stages of train, after which they start behaving \"normally\"</br>\n5. seqeval python module for ner evaluation entity level\n</br>\n6. Due to the padding done with DataCollator, the output \"labels\" of prediction would have lot of -100 values which are to get discarded so as to focus on \"actual\" input token and \"actual corresponding output value\"</br>\n7. My initial understanding of adjusting labels after tokenization is wrong, if word=\"XYZ\" and token=\"XY\" and \"##Z\" and original label for word=\"B-LOC\", then adjusted labels would be \"B-LOC\", \"B-LOC\" and not \"B-LOC\",\"I-LOC\"\n<h2><b>Step 1 : Load the dataset</b></h2>","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"wikiann\", \"bn\")","metadata":{"execution":{"iopub.status.busy":"2022-01-15T10:22:15.410471Z","iopub.execute_input":"2022-01-15T10:22:15.410746Z","iopub.status.idle":"2022-01-15T10:22:32.264464Z","shell.execute_reply.started":"2022-01-15T10:22:15.410717Z","shell.execute_reply":"2022-01-15T10:22:32.263727Z"}},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/3.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d0a75aad95d41c28b3f96d9d64685d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/12.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72ab837ec1b74e1d92f2e3d072ea0d4f"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset wikiann/bn (download: 223.17 MiB, generated: 2.70 MiB, post-processed: Unknown size, total: 225.86 MiB) to /root/.cache/huggingface/datasets/wikiann/bn/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/234M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2543a9d21d6434ca86b530ec7e15317"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset wikiann downloaded and prepared to /root/.cache/huggingface/datasets/wikiann/bn/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7654eed78b064cfd919400718322a0bd"}},"metadata":{}}]},{"cell_type":"markdown","source":"<h2><b>Step 2 : Get list of label names </b></h2>","metadata":{}},{"cell_type":"code","source":"label_names = dataset[\"train\"].features[\"ner_tags\"].feature.names\nlabel_names","metadata":{"execution":{"iopub.status.busy":"2022-01-15T10:22:38.919912Z","iopub.execute_input":"2022-01-15T10:22:38.920170Z","iopub.status.idle":"2022-01-15T10:22:38.929080Z","shell.execute_reply.started":"2022-01-15T10:22:38.920139Z","shell.execute_reply":"2022-01-15T10:22:38.928239Z"}},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"},"metadata":{}}]},{"cell_type":"markdown","source":"<h2><b>Step 3 : Tokenize dataset and adjust the labels</b></h2>\n<ul>\n  <li>encode method returns the required keys (input_ids, token_type_ids, attention_mask) but its not a Pytorch/hf dataset</li>\n  <li>map allows adding new keys to existing splits of the hf dataset, so that's why map is used, no need to make a new dataset from the output of encode</li>\n  <li>adjustment of labels needed as token like \"Johnemma\" will be split into \"john\" and \"##emma\" but label for it would still be \"B-PER\", to align it \"john\" will get label \"B-PER\" and ##emma gets label \"B-PER\"</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n\n#Get the values for input_ids, token_type_ids, attention_mask\ndef tokenize_adjust_labels(all_samples_per_split):\n  tokenized_samples = tokenizer.batch_encode_plus(all_samples_per_split[\"tokens\"], is_split_into_words=True)\n  #tokenized_samples is not a datasets object so this alone won't work with Trainer API, hence map is used \n  #so the new keys [input_ids, labels (after adjustment)]\n  #can be added to the datasets dict for each train test validation split\n  total_adjusted_labels = []\n  print(len(tokenized_samples[\"input_ids\"]))\n  for k in range(0, len(tokenized_samples[\"input_ids\"])):\n    prev_wid = -1\n    word_ids_list = tokenized_samples.word_ids(batch_index=k)\n    existing_label_ids = all_samples_per_split[\"ner_tags\"][k]\n    i = -1\n    adjusted_label_ids = []\n    '''print(word_ids_list)\n    print(existing_label_ids)\n    print(all_samples_per_split[\"tokens\"][k])\n    print(tokenized_samples[\"input_ids\"][k])'''\n    for wid in word_ids_list:\n      if(wid is None):\n        adjusted_label_ids.append(-100)\n      elif(wid!=prev_wid):\n        i = i + 1\n        adjusted_label_ids.append(existing_label_ids[i])\n        prev_wid = wid\n      else:\n        label_name = label_names[existing_label_ids[i]]\n        '''if(label_name == \"O\"):\n          adjusted_label_ids.append(existing_label_ids[i])\n        elif(label_name[0:2]==\"B-\"):\n          adjusted_label_ids.append(label_names.index(label_name.replace(\"B-\",\"I-\")))\n        else:\n          adjusted_label_ids.append(existing_label_ids[i])'''\n        adjusted_label_ids.append(existing_label_ids[i])\n        \n    total_adjusted_labels.append(adjusted_label_ids)\n  tokenized_samples[\"labels\"] = total_adjusted_labels\n  return tokenized_samples\n\ntokenized_dataset = dataset.map(tokenize_adjust_labels, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-15T10:23:05.404913Z","iopub.execute_input":"2022-01-15T10:23:05.405184Z","iopub.status.idle":"2022-01-15T10:23:19.392441Z","shell.execute_reply.started":"2022-01-15T10:23:05.405153Z","shell.execute_reply":"2022-01-15T10:23:19.391728Z"}},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"627a31b2966440dc849e46e9da3e23ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"610716ac94b04d7a8761cbf65d7811c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe83d19a59894937926672c3eacb2fe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.87M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84fc0b18b1fb49b1a4cc545304f9fd69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fec99a77f2d0448096c1d26ab0709d46"}},"metadata":{}},{"name":"stdout","text":"1000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81f878db984a4d75b0da04bfc4bead03"}},"metadata":{}},{"name":"stdout","text":"1000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a483168611046378c1b1f879644bb87"}},"metadata":{}},{"name":"stdout","text":"1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n1000\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized_dataset[\"validation\"][2]","metadata":{"execution":{"iopub.status.busy":"2022-01-15T10:23:27.954828Z","iopub.execute_input":"2022-01-15T10:23:27.955091Z","iopub.status.idle":"2022-01-15T10:23:27.962665Z","shell.execute_reply.started":"2022-01-15T10:23:27.955063Z","shell.execute_reply":"2022-01-15T10:23:27.961992Z"}},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n 'input_ids': [101,\n  978,\n  12235,\n  38044,\n  40349,\n  52245,\n  950,\n  21790,\n  12079,\n  89362,\n  77045,\n  117,\n  978,\n  12235,\n  38044,\n  40349,\n  102],\n 'labels': [-100, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 0, 5, 5, 5, 5, -100],\n 'langs': ['bn', 'bn', 'bn', 'bn', 'bn'],\n 'ner_tags': [5, 6, 6, 0, 5],\n 'spans': ['LOC: সিডনি ক্রিকেট গ্রাউন্ড', 'LOC: সিডনি'],\n 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n 'tokens': ['সিডনি', 'ক্রিকেট', 'গ্রাউন্ড', ',', 'সিডনি']}"},"metadata":{}}]},{"cell_type":"markdown","source":"<h2><b>Step 4 : Pad the samples per split</b></h2>\n<ul>\n  <li>Each token list per sample will be split</li>\n  <li>Sample x and sample y may not have same length so padding is needed\n  <li>This will be used by Trainer API, this is the collate_fn equivalent from pytorch</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T10:23:41.806157Z","iopub.execute_input":"2022-01-15T10:23:41.806849Z","iopub.status.idle":"2022-01-15T10:23:42.540665Z","shell.execute_reply.started":"2022-01-15T10:23:41.806812Z","shell.execute_reply":"2022-01-15T10:23:42.539859Z"}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<h2><b>Step 5 : Set up integration with Weights and Biases </b></h2>","metadata":{}},{"cell_type":"code","source":"!pip install wandb\n!pip install seqeval","metadata":{"execution":{"iopub.status.busy":"2022-01-15T10:23:54.755832Z","iopub.execute_input":"2022-01-15T10:23:54.756636Z","iopub.status.idle":"2022-01-15T10:24:14.585132Z","shell.execute_reply.started":"2022-01-15T10:23:54.756596Z","shell.execute_reply":"2022-01-15T10:24:14.584313Z"}},"execution_count":7,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.12.9)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.26.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.5.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (6.0)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.24)\nRequirement already satisfied: yaspin>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.1.0)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.16.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.8.0)\nRequirement already satisfied: subprocess32>=3.5.3 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.5.4)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.19.1)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.0.3)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.8.0)\nRequirement already satisfied: configparser>=3.8.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.2.0)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.8)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.10.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.0.8)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.7)\nRequirement already satisfied: termcolor<2.0.0,>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from yaspin>=1.0.0->wandb) (1.1.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.6.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n     |████████████████████████████████| 43 kB 197 kB/s            \n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.19.5)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval) (0.23.2)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=e97a3acb615b55f02551d30c7dd53d609133960fe74ef0ba450f45c1f5e554ae\n  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport wandb\nos.environ[\"WANDB_API_KEY\"]=\"API KEY GOES HERE\"\nos.environ[\"WANDB_ENTITY\"]=\"Suchandra\"\nos.environ[\"WANDB_PROJECT\"]=\"ner_project\"","metadata":{"execution":{"iopub.status.busy":"2022-01-15T10:24:28.840218Z","iopub.execute_input":"2022-01-15T10:24:28.840910Z","iopub.status.idle":"2022-01-15T10:24:29.267318Z","shell.execute_reply.started":"2022-01-15T10:24:28.840867Z","shell.execute_reply":"2022-01-15T10:24:29.266577Z"}},"execution_count":8,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h2><b>Step 6 : Load model, define training_args, train</b></h2>","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\nimport numpy as np\nfrom datasets import load_metric\nmetric = load_metric(\"seqeval\")\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    flattened_results = {\n        \"overall_precision\": results[\"overall_precision\"],\n        \"overall_recall\": results[\"overall_recall\"],\n        \"overall_f1\": results[\"overall_f1\"],\n        \"overall_accuracy\": results[\"overall_accuracy\"],\n    }\n    for k in results.keys():\n      if(k not in flattened_results.keys()):\n        flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n\n    return flattened_results\n\nmodel = AutoModelForTokenClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(label_names))\ntraining_args = TrainingArguments(\n    output_dir=\"./jan_14_all_2022\",\n    evaluation_strategy=\"steps\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=7,\n    weight_decay=0.01,\n    logging_steps = 1000,\n    report_to=\"wandb\",\n    run_name = \"ep_10_tokenized_11\",\n    save_strategy='no'\n)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T10:25:13.641211Z","iopub.execute_input":"2022-01-15T10:25:13.641843Z","iopub.status.idle":"2022-01-15T10:33:36.282763Z","shell.execute_reply.started":"2022-01-15T10:25:13.641805Z","shell.execute_reply":"2022-01-15T10:33:36.282029Z"}},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3f7efa5107c42e7a5391549d7568c66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/681M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba04c12e7420425192c5135180612b6d"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: spans, ner_tags, tokens, langs.\n***** Running training *****\n  Num examples = 10000\n  Num Epochs = 7\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4375\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msuchandra\u001b[0m (use `wandb login --relogin` to force relogin)\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                    Syncing run <strong><a href=\"https://wandb.ai/suchandra/ner_project/runs/200gzvyr\" target=\"_blank\">ep_10_tokenized_11</a></strong> to <a href=\"https://wandb.ai/suchandra/ner_project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4375' max='4375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4375/4375 07:41, Epoch 7/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Overall Precision</th>\n      <th>Overall Recall</th>\n      <th>Overall F1</th>\n      <th>Overall Accuracy</th>\n      <th>Loc F1</th>\n      <th>Org F1</th>\n      <th>Per F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>0.336600</td>\n      <td>0.210477</td>\n      <td>0.940126</td>\n      <td>0.941262</td>\n      <td>0.940693</td>\n      <td>0.954287</td>\n      <td>0.934716</td>\n      <td>0.924310</td>\n      <td>0.961162</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.109800</td>\n      <td>0.169222</td>\n      <td>0.954078</td>\n      <td>0.964225</td>\n      <td>0.959125</td>\n      <td>0.967807</td>\n      <td>0.956601</td>\n      <td>0.938577</td>\n      <td>0.978800</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.044700</td>\n      <td>0.144407</td>\n      <td>0.968457</td>\n      <td>0.972202</td>\n      <td>0.970326</td>\n      <td>0.977293</td>\n      <td>0.968137</td>\n      <td>0.956946</td>\n      <td>0.983772</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.020500</td>\n      <td>0.132434</td>\n      <td>0.968901</td>\n      <td>0.971477</td>\n      <td>0.970187</td>\n      <td>0.977965</td>\n      <td>0.969212</td>\n      <td>0.956831</td>\n      <td>0.982079</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: spans, ner_tags, tokens, langs.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nThe following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: spans, ner_tags, tokens, langs.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nThe following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: spans, ner_tags, tokens, langs.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nThe following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: spans, ner_tags, tokens, langs.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br/>Waiting for W&B process to finish, PID 315... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/LOC_f1</td><td>▁▅██</td></tr><tr><td>eval/ORG_f1</td><td>▁▄██</td></tr><tr><td>eval/PER_f1</td><td>▁▆█▇</td></tr><tr><td>eval/loss</td><td>█▄▂▁</td></tr><tr><td>eval/overall_accuracy</td><td>▁▅██</td></tr><tr><td>eval/overall_f1</td><td>▁▅██</td></tr><tr><td>eval/overall_precision</td><td>▁▄██</td></tr><tr><td>eval/overall_recall</td><td>▁▆██</td></tr><tr><td>eval/runtime</td><td>▂▆▁█</td></tr><tr><td>eval/samples_per_second</td><td>▇▂█▁</td></tr><tr><td>eval/steps_per_second</td><td>▇▂█▁</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▅▅▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▅▅▇▇█</td></tr><tr><td>train/learning_rate</td><td>█▆▃▁</td></tr><tr><td>train/loss</td><td>█▃▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/LOC_f1</td><td>0.96921</td></tr><tr><td>eval/ORG_f1</td><td>0.95683</td></tr><tr><td>eval/PER_f1</td><td>0.98208</td></tr><tr><td>eval/loss</td><td>0.13243</td></tr><tr><td>eval/overall_accuracy</td><td>0.97797</td></tr><tr><td>eval/overall_f1</td><td>0.97019</td></tr><tr><td>eval/overall_precision</td><td>0.9689</td></tr><tr><td>eval/overall_recall</td><td>0.97148</td></tr><tr><td>eval/runtime</td><td>2.2492</td></tr><tr><td>eval/samples_per_second</td><td>444.598</td></tr><tr><td>eval/steps_per_second</td><td>28.01</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>4375</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0205</td></tr><tr><td>train/total_flos</td><td>1092890424498048.0</td></tr><tr><td>train/train_loss</td><td>0.11807</td></tr><tr><td>train/train_runtime</td><td>471.0513</td></tr><tr><td>train/train_samples_per_second</td><td>148.604</td></tr><tr><td>train/train_steps_per_second</td><td>9.288</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">ep_10_tokenized_11</strong>: <a href=\"https://wandb.ai/suchandra/ner_project/runs/200gzvyr\" target=\"_blank\">https://wandb.ai/suchandra/ner_project/runs/200gzvyr</a><br/>\nFind logs at: <code>./wandb/run-20220115_102539-200gzvyr/logs</code><br/>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"<h2><b>Step 7 : Check performance on test set</b></h2>","metadata":{}},{"cell_type":"code","source":"predictions, labels, _ = trainer.predict(tokenized_dataset[\"test\"])\npredictions = np.argmax(predictions, axis=2)\n#Here we will see that the labels list will have lots of -100 in them however the corresponding label of the \n#tokenized_dataset doesnt have it, the reason is during DataCollator padding step, all padding tokens are added\n#and assigned labels of -100 to get \"ignored\" in future computation of evaluation\n\n# Remove ignored index (special tokens)\ntrue_predictions = [\n    [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n    for prediction, label in zip(predictions, labels)\n]\ntrue_labels = [\n    [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n    for prediction, label in zip(predictions, labels)\n]\n\nresults = metric.compute(predictions=true_predictions, references=true_labels)\nresults","metadata":{"execution":{"iopub.status.busy":"2022-01-15T10:34:17.251023Z","iopub.execute_input":"2022-01-15T10:34:17.251327Z","iopub.status.idle":"2022-01-15T10:34:19.938826Z","shell.execute_reply.started":"2022-01-15T10:34:17.251294Z","shell.execute_reply":"2022-01-15T10:34:19.938139Z"}},"execution_count":10,"outputs":[{"name":"stderr","text":"The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: spans, ner_tags, tokens, langs.\n***** Running Prediction *****\n  Num examples = 1000\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:01]\n    </div>\n    "},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'LOC': {'precision': 0.9700199866755497,\n  'recall': 0.9642384105960264,\n  'f1': 0.967120557954168,\n  'number': 1510},\n 'ORG': {'precision': 0.9548335974643423,\n  'recall': 0.9725585149313963,\n  'f1': 0.9636145541783286,\n  'number': 1239},\n 'PER': {'precision': 0.9716629381058911,\n  'recall': 0.9702159344750558,\n  'f1': 0.9709388971684053,\n  'number': 1343},\n 'overall_precision': 0.9658869395711501,\n 'overall_recall': 0.9687194525904204,\n 'overall_f1': 0.96730112249878,\n 'overall_accuracy': 0.9744194618503501}"},"metadata":{}}]},{"cell_type":"markdown","source":"<h2><b>Step 8 : Save model for future use</b></h2>","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"jan_14_all_2022\")","metadata":{"execution":{"iopub.status.busy":"2022-01-15T10:35:39.175573Z","iopub.execute_input":"2022-01-15T10:35:39.176274Z","iopub.status.idle":"2022-01-15T10:35:40.616942Z","shell.execute_reply.started":"2022-01-15T10:35:39.176232Z","shell.execute_reply":"2022-01-15T10:35:40.616233Z"}},"execution_count":11,"outputs":[{"name":"stderr","text":"Configuration saved in jan_14_all_2022/config.json\nModel weights saved in jan_14_all_2022/pytorch_model.bin\n","output_type":"stream"}]},{"cell_type":"code","source":"saved_model = AutoModelForTokenClassification.from_pretrained(\"jan_14_all_2022\")\nsaved_model","metadata":{"execution":{"iopub.status.busy":"2022-01-15T10:35:50.120468Z","iopub.execute_input":"2022-01-15T10:35:50.120744Z","iopub.status.idle":"2022-01-15T10:35:52.447752Z","shell.execute_reply.started":"2022-01-15T10:35:50.120714Z","shell.execute_reply":"2022-01-15T10:35:52.446796Z"}},"execution_count":12,"outputs":[{"name":"stderr","text":"loading configuration file jan_14_all_2022/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"bert-base-multilingual-cased\",\n  \"architectures\": [\n    \"BertForTokenClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\",\n    \"4\": \"LABEL_4\",\n    \"5\": \"LABEL_5\",\n    \"6\": \"LABEL_6\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4,\n    \"LABEL_5\": 5,\n    \"LABEL_6\": 6\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.12.5\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 119547\n}\n\nloading weights file jan_14_all_2022/pytorch_model.bin\nAll model checkpoint weights were used when initializing BertForTokenClassification.\n\nAll the weights of BertForTokenClassification were initialized from the model checkpoint at jan_14_all_2022.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=7, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"<h2><b>Step 9 : Predict on random sentences, no longer using Trainer API, model inputs expected to be tensors</b></h2>","metadata":{}},{"cell_type":"code","source":"import torch\nrandom_sentence_from_internet = [ \"মারভিন\", \"দি\", \"মারসিয়ান\" ]  \ninput = tokenizer(random_sentence_from_internet, is_split_into_words=True, return_tensors='pt')\nprint(input)\noutput = saved_model(**input)\npredictions = torch.nn.functional.softmax(output.logits, dim=-1)\npredictions = predictions.detach().numpy()\npredictions = np.argmax(predictions, axis=2)\nprint(predictions)\n\npred_names = [label_names[p] for p in predictions[0]]\nfor index, id in enumerate(input[\"input_ids\"][0]):\n  print(\"\\nID: \", id, \"Decoded ID: \", tokenizer.decode(id), \"\\tPred: \", pred_names[index])","metadata":{"execution":{"iopub.status.busy":"2022-01-15T10:57:43.325577Z","iopub.execute_input":"2022-01-15T10:57:43.325864Z","iopub.status.idle":"2022-01-15T10:57:43.467371Z","shell.execute_reply.started":"2022-01-15T10:57:43.325833Z","shell.execute_reply":"2022-01-15T10:57:43.466648Z"}},"execution_count":19,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[  101, 18601, 11128, 80045, 11737,   965, 12235, 18601, 11128, 45733,\n         96032,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n[[2 1 1 1 1 2 2 2 2 2 2 2]]\n\nID:  tensor(101) Decoded ID:  [CLS] \tPred:  I-PER\n\nID:  tensor(18601) Decoded ID:  মা \tPred:  B-PER\n\nID:  tensor(11128) Decoded ID:  ##র \tPred:  B-PER\n\nID:  tensor(80045) Decoded ID:  ##ভি \tPred:  B-PER\n\nID:  tensor(11737) Decoded ID:  ##ন \tPred:  B-PER\n\nID:  tensor(965) Decoded ID:  দ \tPred:  I-PER\n\nID:  tensor(12235) Decoded ID:  ##ি \tPred:  I-PER\n\nID:  tensor(18601) Decoded ID:  মা \tPred:  I-PER\n\nID:  tensor(11128) Decoded ID:  ##র \tPred:  I-PER\n\nID:  tensor(45733) Decoded ID:  ##সি \tPred:  I-PER\n\nID:  tensor(96032) Decoded ID:  ##য়ান \tPred:  I-PER\n\nID:  tensor(102) Decoded ID:  [SEP] \tPred:  I-PER\n","output_type":"stream"}]}]}